# Extractra ğŸŒ

> Intelligent Multi-Agent Web Data Extraction System - Built with Google Agent Development Kit (ADK)

## ğŸ“– Project Overview

Extractra is an intelligent web data extraction platform that uses a multi-agent system to automatically discover, extract, and integrate website data. Through the Google ADK framework, the system can understand user requirements, intelligently analyze website structures, and provide structured data output.

### ğŸ¯ Core Features

- **Intelligent Page Discovery**: Automatically analyze website structure and discover relevant pages
- **Parallel Content Extraction**: Multi-agent parallel processing for efficient data extraction
- **Smart Data Integration**: Automatic deduplication, cleaning, and data formatting
- **Real-time Progress Tracking**: WebSocket real-time extraction progress display
- **CSV Export**: Support for exporting structured data to CSV format

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

- **Node.js**: 18+ 
- **Python**: 3.11+
- **Docker**: 20.10+ (optional)
- **Google Cloud**: ADK service configuration

### ğŸ”§ Local Development

#### 1. Clone the Repository

```bash
git clone https://github.com/your-username/extractra.git
cd extractra
```

#### 2. Backend Setup

```bash
# Navigate to backend directory
cd backend

# Install dependencies
pip install -r requirements.txt

# Configure environment variables
# - GOOGLE_CLOUD_PROJECT
# - GOOGLE_CLOUD_LOCATION
# - PD_GEMINI_MODEL
# - CE_GEMINI_MODEL

# Install Playwright browsers
playwright install chromium

# Start backend service
python main.py
```

Backend service will start at `http://localhost:8080`

#### 3. Frontend Setup

```bash
# Navigate to frontend directory
cd fronted

# Install dependencies
npm install
# or use pnpm
pnpm install

# Configure environment variables
# VITE_API_BASE_URL=http://localhost:8080
# VITE_WS_BASE_URL=ws://localhost:8080

# Start development server
npm run dev 
# or use pnpm
pnpm dev
```

Frontend application will start at `http://localhost:8080`

### ğŸ³ Docker Deployment

#### Build and Run Backend

```bash
# Build backend image from project root
docker build -f backend/Dockerfile -t extractra-backend .

# Run backend container
docker run -d \
  -p 8080:8080 \
  -e GOOGLE_CLOUD_PROJECT=your-project \
  -e GOOGLE_CLOUD_LOCATION=us-central1 \
  --name extractra-backend \
  extractra-backend
```

#### Build and Run Frontend

```bash
# Build frontend image
cd fronted
docker build \
  --build-arg VITE_API_BASE_URL=http://localhost:8080 \
  --build-arg VITE_WS_BASE_URL=ws://localhost:8080 \
  -t extractra-frontend .

# Run frontend container
docker run -d \
  -p 3000:8080 \
  --name extractra-frontend \
  extractra-frontend
```

## ğŸ“š Usage Guide

### 1. Create Extraction Task

1. Visit Extractra homepage
2. Enter target website URL
3. Describe the data you want to extract
4. Click "Start Extraction"

### 2. Monitor Extraction Progress

The system displays progress across three stages:
- **Page Discovery**: Analyze website structure and discover relevant pages
- **Content Extraction**: Parallel extraction of data from pages
- **Result Integration**: Clean and integrate data

### 3. Download Results

After extraction completion, you can:
- Preview data table
- Download CSV file
- View extraction statistics

## ğŸ› ï¸ Technology Stack

### Frontend
- **React 18** + **TypeScript**
- **Vite** - Fast build tool
- **shadcn/ui** - Modern UI component library
- **Tailwind CSS** - Utility-first CSS framework
- **React Router** - Client-side routing
- **WebSocket** - Real-time communication

### Backend
- **FastAPI** - Modern Python web framework
- **Google ADK** - Agent Development Kit
- **Crawl4AI** - Web scraping and content extraction
- **Playwright** - Browser automation
- **Pydantic** - Data validation and serialization

### Deployment
- **Docker** - Containerized deployment
- **Google Cloud Run** - Serverless container platform
- **Google Cloud Build** - CI/CD pipeline

## ğŸ”§ Configuration

### Backend Environment Variables

```bash
# Google Cloud Configuration
GOOGLE_CLOUD_PROJECT=your-project-id
GOOGLE_CLOUD_LOCATION=location

# LLM Model Configuration
PD_GEMINI_MODEL=gemini-2.0-flash-exp
CE_GEMINI_MODEL=gemini-2.0-flash-exp
GOOGLE_GENAI_USE_VERTEXAI=true

# Service Configuration
HOST=0.0.0.0
PORT=8080
DEBUG=false
LOG_LEVEL=INFO

# Frontend Domain (CORS Configuration)
FRONTED_DOMAIN=https://your-frontend-domain.com
```

### Frontend Environment Variables

```bash
# API Configuration
VITE_API_BASE_URL=http://localhost:8080
VITE_WS_BASE_URL=ws://localhost:8080
```

## ğŸ“ Project Structure

```
Extractra/
â”œâ”€â”€ backend/                 # Backend service
â”‚   â”œâ”€â”€ adk/                # ADK agent system
â”‚   â”‚   â”œâ”€â”€ agents.py       # Agent definitions
â”‚   â”‚   â””â”€â”€ tools/          # Tool modules
â”‚   â”œâ”€â”€ api/                # API routes
â”‚   â”‚   â””â”€â”€ v1/            # API v1 version
â”‚   â”œâ”€â”€ core/              # Core configuration
â”‚   â”œâ”€â”€ models/            # Data models
â”‚   â”œâ”€â”€ services/          # Business services
â”‚   â”œâ”€â”€ main.py            # Application entry point
â”‚   â”œâ”€â”€ requirements.txt   # Python dependencies
â”‚   â””â”€â”€ Dockerfile         # Docker configuration
â”œâ”€â”€ fronted/               # Frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â”‚   â”œâ”€â”€ hooks/         # Custom hooks
â”‚   â”‚   â”œâ”€â”€ lib/           # Utility libraries
â”‚   â”‚   â””â”€â”€ types/         # Type definitions
â”‚   â”œâ”€â”€ public/            # Static assets
â”‚   â”œâ”€â”€ package.json       # Frontend dependencies
â”‚   â””â”€â”€ Dockerfile         # Docker configuration
â”œâ”€â”€ doc/                   # Project documentation
â”‚   â”œâ”€â”€ prd.md            # Product requirements
â”‚   â””â”€â”€ tech.md           # Technical architecture
â””â”€â”€ README.md             # Project documentation
```

This project uses Crawl4AI (https://github.com/unclecode/crawl4ai) for web crawling and markdown conversion.

**Extractra** - Making web data extraction intelligent and simple âœ¨